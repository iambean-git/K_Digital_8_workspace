{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3KD7xHe2vNc"
      },
      "source": [
        "# 0. 자신이 사용하는 torch 버전과 GPU 사용 가능 여부를 확인하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Xgl77Km2t0F",
        "outputId": "5d569ce3-6533-4fd5-de43-ff726503e4f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.2.2'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "relTZHRB22m8",
        "outputId": "d781e123-964c-499e-c39c-48446b6b4265"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5W6bnNp2MoG"
      },
      "source": [
        "# 1. input_size 가 1이고, hidden_size 10인 RNN 객체를 생성하시오. (1, 1, 1) 데이터를 torch.rand 로 생성 후, RNN 객체에 전달하여 출력값을 확인하시오. (데이터 값, shape)\n",
        "\n",
        "* RNN 객체 생성 시 batch 데이터의 위치를 고려하시오.\n",
        "* RNN 객체의 반환값 형태에 주의하시오. (공식 문서 참고)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "rnn = nn.RNN(input_size=1, hidden_size=10, batch_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "input = torch.rand(1,1,1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8QwqpEW1MJJ",
        "outputId": "be6aee8f-19e2-48ed-91da-1e15d5220360"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[-0.0459, -0.2034,  0.2793, -0.4276,  0.3874, -0.2163, -0.2090,\n",
              "           -0.1898, -0.2740,  0.1504]]], grad_fn=<TransposeBackward1>),\n",
              " tensor([[[-0.0459, -0.2034,  0.2793, -0.4276,  0.3874, -0.2163, -0.2090,\n",
              "           -0.1898, -0.2740,  0.1504]]], grad_fn=<StackBackward0>))"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y, h = rnn(input)\n",
        "y, h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([1, 1, 10]), torch.Size([1, 1, 10]))"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.shape, h.shape "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phA0iH892-08"
      },
      "source": [
        "# 2. input_size 가 2이고, hidden_size 5인 RNN 객체를 생성하시오. (1, 1, 2) 데이터를 torch.rand 로 생성 후, RNN 객체에 전달하여 출력값을 확인하시오. (데이터 값, shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1gVN6B_3FVn",
        "outputId": "b0ff6e10-a3c7-480b-8417-b966d83933fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([1, 1, 5]), torch.Size([1, 1, 5]))"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rnn2 = nn.RNN(input_size=2, hidden_size=5, batch_first=True)\n",
        "input2 = torch.rand(1,1,2)\n",
        "y2, h2 = rnn2(input2)\n",
        "y2.shape, h2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdbFkU5C3G6y",
        "outputId": "d82c8f1b-05b1-422f-98be-819b0c2ee21f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[-0.5110,  0.5381, -0.3635, -0.5548, -0.3754]]],\n",
              "        grad_fn=<TransposeBackward1>),\n",
              " tensor([[[-0.5110,  0.5381, -0.3635, -0.5548, -0.3754]]],\n",
              "        grad_fn=<StackBackward0>))"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y2, h2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o2cWJCV3MJA"
      },
      "source": [
        "# 3. input_size 가 1이고, hidden_size 10인 LSTM 객체를 생성하시오. (1, 1, 1) 데이터를 torch.rand 로 생성 후, LSTM 객체에 전달하여 출력값을 확인하시오. (데이터 값, shape)\n",
        "\n",
        "* LSTM 객체 생성 시 batch 데이터의 위치를 고려하시오.\n",
        "* LSTM 객체의 반환값 형태에 주의하시오. (공식 문서 참고)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Evt9Y21z3Uh2",
        "outputId": "fe7754a7-43d2-4dd2-ea28-cb1ee1faa8b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([1, 1, 10]), torch.Size([1, 1, 10]), torch.Size([1, 1, 10]))"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstm3 = nn.LSTM(input_size=1, hidden_size=10, batch_first=True)\n",
        "input3 = torch.rand(1,1,1)\n",
        "y3, (h3, c3) = lstm3(input3)\n",
        "y3.shape, h3.shape, c3.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILCL-zca3V4N",
        "outputId": "3bb2681b-43f4-4a30-854b-208f238bf360"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[-0.0887,  0.0464, -0.0045,  0.0522,  0.0112, -0.0350,  0.0548,\n",
              "            0.0042, -0.0274,  0.1015]]], grad_fn=<TransposeBackward0>),\n",
              " tensor([[[-0.0887,  0.0464, -0.0045,  0.0522,  0.0112, -0.0350,  0.0548,\n",
              "            0.0042, -0.0274,  0.1015]]], grad_fn=<StackBackward0>),\n",
              " tensor([[[-0.1834,  0.0819, -0.0087,  0.0981,  0.0224, -0.0912,  0.1206,\n",
              "            0.0082, -0.0432,  0.2440]]], grad_fn=<StackBackward0>))"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y3, h3, c3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqHj1xY73iLM"
      },
      "source": [
        "# 4. input_size 가 2이고, hidden_size 5인 LSTM 객체를 생성하시오. (1, 1, 2) 데이터를 torch.rand 로 생성 후, LSTM 객체에 전달하여 출력값을 확인하시오. (데이터 값, shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsYFH4D33Zj-",
        "outputId": "df36c195-03f3-450a-84d0-04e2f0796aba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([1, 1, 5]), torch.Size([1, 1, 5]), torch.Size([1, 1, 5]))"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstm4 = nn.LSTM(input_size=2, hidden_size=5, batch_first=True)\n",
        "input4 = torch.rand(1,1,2)\n",
        "out = lstm4(input4)\n",
        "y4, (h4, c4)= lstm4(input4)\n",
        "y4.shape, h4.shape, c4.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JfOzctd30Ga",
        "outputId": "fb722173-ab79-4a1e-d8de-ffbf7cb355a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[-0.0564,  0.1993,  0.0305,  0.1323,  0.0359]]],\n",
              "        grad_fn=<TransposeBackward0>),\n",
              " tensor([[[-0.0564,  0.1993,  0.0305,  0.1323,  0.0359]]],\n",
              "        grad_fn=<StackBackward0>),\n",
              " tensor([[[-0.1269,  0.5151,  0.0615,  0.2955,  0.0711]]],\n",
              "        grad_fn=<StackBackward0>))"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y4, h4, c4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-rxn_C933q8"
      },
      "source": [
        "# 5. input_size 가 1 이고 seq_len 이 30 인 입력 데이터를 torch.rand 로 생성하시오. 3번에서 생성한 LSTM 객체를 이용하여 LSTM 객체에 전달하여 출력값을 확인하시오 (shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vcfo8lBo31dh",
        "outputId": "1ceb73ae-1592-468a-ad75-0dcf462b0171"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([1, 30, 10]), torch.Size([1, 1, 10]), torch.Size([1, 1, 10]))"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input5 = torch.rand(1, 30, 1)\n",
        "y5, (h5, c5) = lstm3(input5)\n",
        "y5.shape, h5.shape, c5.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUbbZ93i4lwM"
      },
      "source": [
        "# 6. input_size 가 2 이고 seq_len 이 60 인 입력 데이터를 torch.rand 로 생성하시오. 4번에서 생성한 LSTM 객체를 이용하여 LSTM 객체에 전달하여 출력값을 확인하시오 (shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "344ZjsEJ4jlV",
        "outputId": "07bdbfcd-9d9c-4368-bfa3-f03b059fabf0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([1, 60, 5]), torch.Size([1, 1, 5]), torch.Size([1, 1, 5]))"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input6 = torch.rand(1, 60, 2)\n",
        "y6, (h6, c6) = lstm4(input6)\n",
        "y6.shape, h6.shape, c6.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_hSd57h45o_"
      },
      "source": [
        "# 7. 순환신경망에서 many-to-one 모델은 여러 개의 입력을 받아 하나의 값을 출력하는 구조이다.\n",
        "\n",
        "* 예를 들어, 스타벅스 주가를 예측하려고 할 때, **과거 30일의 주가 데이터**를 이용하여 **내일의 주가**를 예측하는 경우이다.\n",
        "\n",
        "* 예를 참고하여 이에 알맞은 입력 데이터와 LSTM 모델을 생성하고 출력값의 shape 을 확인하세요. (단, batch_size=32, hidden_size=10 이다.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 30, 10]), torch.Size([1, 32, 10]), torch.Size([1, 32, 10]))"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = torch.rand(32, 30, 1)\n",
        "lstm = nn.LSTM(1, 10, batch_first=True)\n",
        "y, (h, c) = lstm(data)\n",
        "y.shape, h.shape, c.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTlLyM3172kz"
      },
      "source": [
        "# 8. 순환신경망에서 many-to-one 모델은 여러 개의 입력을 받아 하나의 값을 출력하는 구조이다.\n",
        "\n",
        "* 예를 들어, 스타벅스 주가를 예측하려고 할 때, **과거 30일의 주가와 거래량 데이터**를 이용하여 **내일의 주가**를 예측하는 경우이다.\n",
        "\n",
        "* 예를 참고하여 이에 알맞은 입력 데이터와 LSTM 모델을 생성하고 출력값의 shape 을 확인하세요. (단, batch_size=32, hidden_size=10 이다.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AOz5_Ge6Ldz",
        "outputId": "cdf01344-9644-4c42-a738-9e86488fbaf6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 30, 10]), torch.Size([1, 32, 10]), torch.Size([1, 32, 10]))"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = torch.rand(32, 30, 2)\n",
        "lstm = nn.LSTM(2, 10, batch_first=True)\n",
        "y, (h, c) = lstm(data)\n",
        "y.shape, h.shape, c.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE7SMjEP8UUi"
      },
      "source": [
        "# 9. 8번의 출력에서 가장 마지막 출력값을 내일의 스타벅스 주가로 정했다. 해당 값을 출력하시오. (slicing 이용)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZKRZB4E8P-y",
        "outputId": "7f5eceef-1d42-4efb-fd06-c6178da5469c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([30, 10])"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y[-1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 10])"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y[:,-1,:].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y54DIPz8rMm"
      },
      "source": [
        "# 10. 9번의 마지막 출력값의 모양이 1이 아니라 10이다. 이를 위해, Linear 레이어를 추가하여 마지막 출력이 (32, 1) 가 되도록 코드를 작성하시오.\n",
        "\n",
        "* 입력데이터 > LSTM > Linear > 출력데이터 순서로 코드를 작성하시오."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc3cWwgl8nEh",
        "outputId": "19afc49a-94de-4333-f9a4-4ba79c7b617d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 1])"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = torch.rand(32, 30, 1)\n",
        "lstm = nn.LSTM(1, 10, batch_first=True)\n",
        "y, (h, c) = lstm(data)\n",
        "ln = nn.Linear(10, 1)\n",
        "ln(y[:,-1,:]).shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLfUO0M49_gC"
      },
      "source": [
        "# 11. 10번에서 LSTM 객체와 Linear 객체를 nn.Sequential 로 하나로 묶어 model 변수에 저장해 보세요. 그런 다음 입력 데이터를 전달하여 출력을 확인해 보세요. 에러가 발생하면 가능한 방법을 고민하여 구현해 보세요. (hint: nn.Module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "gyUToyR79bi8"
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(\n",
        "    nn.LSTM(2, 10, batch_first=True),\n",
        "    nn.Linear(10,1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "linear(): argument 'input' (position 1) must be Tensor, not tuple",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[50], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch_book\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch_book\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch_book\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch_book\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch_book\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch_book\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not tuple"
          ]
        }
      ],
      "source": [
        "# data = torch.rand(32, 30, 2)\n",
        "# model(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "위의 방법은 에러가 남 아래를 보자"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "lstm = nn.LSTM(2, 10, batch_first=True)\n",
        "linear = nn.Linear(10,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StockPrediction(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=10, output_size=1) :\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    \n",
        "    def forward(self, input) :\n",
        "        y, (h, c) = self.lstm(input)\n",
        "        output = self.linear(y[:,-1,:])\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 1])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = StockPrediction()\n",
        "data = torch.rand(32, 30, 1)\n",
        "model(data).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-84SIT0-02o"
      },
      "source": [
        "# 12. yfinance 라이브러리를 이용하여 스타벅스(ticker=SUBX) 주가를 다운로드 합니다. 다음으로 Close 데이터만 data 변수에 저장한 후, type 과 shape 을 확인해 봅니다.\n",
        "\n",
        "* yfinance 데이터는 column 인덱스가 MultiIndex 입니다.\n",
        "\n",
        "```\n",
        "pip install yfinance\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gO1qQfo-rX0",
        "outputId": "b61aa8ff-ed4d-4648-fad0-bda370ae41ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "\n",
        "ticker_symbol = \"SBUX\"\n",
        "df = yf.download(ticker_symbol, start=\"2010-01-01\", end=\"2023-12-31\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th>Price</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Open</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ticker</th>\n",
              "      <th>SBUX</th>\n",
              "      <th>SBUX</th>\n",
              "      <th>SBUX</th>\n",
              "      <th>SBUX</th>\n",
              "      <th>SBUX</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-01-04</th>\n",
              "      <td>8.890197</td>\n",
              "      <td>8.998191</td>\n",
              "      <td>8.836200</td>\n",
              "      <td>8.994334</td>\n",
              "      <td>16370000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-05</th>\n",
              "      <td>9.098470</td>\n",
              "      <td>9.256603</td>\n",
              "      <td>8.828485</td>\n",
              "      <td>8.855483</td>\n",
              "      <td>30058800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-06</th>\n",
              "      <td>9.032902</td>\n",
              "      <td>9.110040</td>\n",
              "      <td>8.971191</td>\n",
              "      <td>9.056043</td>\n",
              "      <td>14209600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-07</th>\n",
              "      <td>9.009761</td>\n",
              "      <td>9.075328</td>\n",
              "      <td>8.944193</td>\n",
              "      <td>8.978906</td>\n",
              "      <td>10256000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-08</th>\n",
              "      <td>8.978904</td>\n",
              "      <td>9.102325</td>\n",
              "      <td>8.944191</td>\n",
              "      <td>8.948048</td>\n",
              "      <td>10274800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Price          Close      High       Low      Open    Volume\n",
              "Ticker          SBUX      SBUX      SBUX      SBUX      SBUX\n",
              "Date                                                        \n",
              "2010-01-04  8.890197  8.998191  8.836200  8.994334  16370000\n",
              "2010-01-05  9.098470  9.256603  8.828485  8.855483  30058800\n",
              "2010-01-06  9.032902  9.110040  8.971191  9.056043  14209600\n",
              "2010-01-07  9.009761  9.075328  8.944193  8.978906  10256000\n",
              "2010-01-08  8.978904  9.102325  8.944191  8.948048  10274800"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmcPg-gCAdkF",
        "outputId": "be959923-bd7b-4434-c9e3-ea271ffd465f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MultiIndex([( 'Close', 'SBUX'),\n",
              "            (  'High', 'SBUX'),\n",
              "            (   'Low', 'SBUX'),\n",
              "            (  'Open', 'SBUX'),\n",
              "            ('Volume', 'SBUX')],\n",
              "           names=['Price', 'Ticker'])"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Date\n",
              "2010-01-04     8.890195\n",
              "2010-01-05     9.098470\n",
              "2010-01-06     9.032902\n",
              "2010-01-07     9.009758\n",
              "2010-01-08     8.978904\n",
              "                ...    \n",
              "2023-12-22    93.432343\n",
              "2023-12-26    93.814774\n",
              "2023-12-27    93.442146\n",
              "2023-12-28    94.069733\n",
              "2023-12-29    94.148186\n",
              "Name: (Close, SBUX), Length: 3522, dtype: float64"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[( 'Close', 'SBUX')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ↑ multi-index 형태라 데이터를 다루기 불편함\n",
        "\n",
        " single-index(단순 string index로 바꾸기)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "_t3h8ydjTMJm"
      },
      "outputs": [],
      "source": [
        "df.columns = ['Close', 'High', 'Low', 'Open', 'Volume' ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Open</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-01-04</th>\n",
              "      <td>8.890197</td>\n",
              "      <td>8.998191</td>\n",
              "      <td>8.836200</td>\n",
              "      <td>8.994334</td>\n",
              "      <td>16370000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-05</th>\n",
              "      <td>9.098470</td>\n",
              "      <td>9.256603</td>\n",
              "      <td>8.828485</td>\n",
              "      <td>8.855483</td>\n",
              "      <td>30058800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-06</th>\n",
              "      <td>9.032902</td>\n",
              "      <td>9.110040</td>\n",
              "      <td>8.971191</td>\n",
              "      <td>9.056043</td>\n",
              "      <td>14209600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-07</th>\n",
              "      <td>9.009761</td>\n",
              "      <td>9.075328</td>\n",
              "      <td>8.944193</td>\n",
              "      <td>8.978906</td>\n",
              "      <td>10256000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-08</th>\n",
              "      <td>8.978904</td>\n",
              "      <td>9.102325</td>\n",
              "      <td>8.944191</td>\n",
              "      <td>8.948048</td>\n",
              "      <td>10274800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Close      High       Low      Open    Volume\n",
              "Date                                                        \n",
              "2010-01-04  8.890197  8.998191  8.836200  8.994334  16370000\n",
              "2010-01-05  9.098470  9.256603  8.828485  8.855483  30058800\n",
              "2010-01-06  9.032902  9.110040  8.971191  9.056043  14209600\n",
              "2010-01-07  9.009761  9.075328  8.944193  8.978906  10256000\n",
              "2010-01-08  8.978904  9.102325  8.944191  8.948048  10274800"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(pandas.core.frame.DataFrame, (3522, 5))"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(df), df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. 30개 Slicing\n",
        "\n",
        "이 30개를 바탕으로 내일(2010-02-17) 주가를 예상해볼 것이다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Date\n",
              "2010-01-04    8.890197\n",
              "2010-01-05    9.098470\n",
              "2010-01-06    9.032902\n",
              "2010-01-07    9.009761\n",
              "2010-01-08    8.978904\n",
              "2010-01-11    8.951912\n",
              "2010-01-12    8.801485\n",
              "2010-01-13    9.017470\n",
              "2010-01-14    9.083041\n",
              "2010-01-15    8.975047\n",
              "2010-01-19    9.094615\n",
              "2010-01-20    8.982763\n",
              "2010-01-21    9.133181\n",
              "2010-01-22    8.836204\n",
              "2010-01-25    8.639498\n",
              "2010-01-26    8.693493\n",
              "2010-01-27    8.643353\n",
              "2010-01-28    8.516077\n",
              "2010-01-29    8.404227\n",
              "2010-02-01    8.573929\n",
              "2010-02-02    8.658781\n",
              "2010-02-03    8.651069\n",
              "2010-02-04    8.411935\n",
              "2010-02-05    8.369512\n",
              "2010-02-08    8.450507\n",
              "2010-02-09    8.558509\n",
              "2010-02-10    8.627922\n",
              "2010-02-11    8.701207\n",
              "2010-02-12    8.716638\n",
              "2010-02-16    8.824627\n",
              "Name: Close, dtype: float64"
            ]
          },
          "execution_count": 200,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Close'].iloc[:30]       # 0~29"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 우리가 예측할 값의 실제값을 미리 살펴보자"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8.928765296936035"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"Close\"].iloc[30]    # 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = df['Close']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVMR4xgk_R66",
        "outputId": "68b154d8-33e4-4e5d-a299-c2cd378a0b2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(pandas.core.series.Series, (3522,))"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(data), data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5Lo-DFL_xEe"
      },
      "source": [
        "# 13. seq_len=30 을 고려하여 X, y dataset 을 생성하는 코드를 작성하세요. 이때, X 는 3d, y 는 2d 로 shape 을 조정해 주세요 (hint: reshape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(numpy.ndarray, (3522,))"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "data_np = data.values       ## values 하면 자동으로 numpy로 바뀜\n",
        "type(data_np), data_np.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((30,), 8.928765296936035)"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seq_len = 30\n",
        "data_np[0:0+seq_len].shape, data_np[0+seq_len]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "한칸 미뤄서 데이터 만들어보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((30,), 8.971189498901367)"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_np[1:1+seq_len].shape, data_np[1+seq_len]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "위를 적용해서 다음 30개 데이터 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "l8wWed18_lxe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "data_np = data.values\n",
        "\n",
        "seq_len = 30\n",
        "X, y = [], []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3522,)"
            ]
          },
          "execution_count": 255,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_np.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(data)-seq_len) :\n",
        "    X.append(data_np[i:i+seq_len])\n",
        "    y.append(data_np[i+seq_len])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((3492, 30), (3492,))"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array(X).shape, np.array(y).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = np.array(X).reshape(-1, seq_len, 1)\n",
        "y = np.array(y).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDPEyBtsBCkp",
        "outputId": "148ac337-7b68-493a-c56c-0c39e90624d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(numpy.ndarray, numpy.ndarray, (3492, 30, 1), (3492, 1))"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(X), type(y), X.shape, y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3klCQcKB8XM"
      },
      "source": [
        "# 14. 13번에서 생성한 데이터셋을 이용하여 DataLoder 객체를 생성하고, 11번을 참고하여 데이터에 알맞은 모델을 생성합니다 (hidden_size=10)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "P9-OG9IBC8Bi"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "X_data, y_data = torch.tensor(X).float(), torch.tensor(y).float()\n",
        "loader_train = DataLoader(TensorDataset(X_data, y_data), batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StockPrediction(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=10, output_size=1) :\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    \n",
        "    def forward(self, input) :\n",
        "        y, (h, c) = self.lstm(input)\n",
        "        output = self.linear(y[:,-1,:])\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "GtkCQEHfBdjU"
      },
      "outputs": [],
      "source": [
        "model = StockPrediction()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsl9DgoAC6mY",
        "outputId": "9e19354d-ca3c-46b4-d2fb-6ac21b91363b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 1])\n"
          ]
        }
      ],
      "source": [
        "for I, label in loader_train:\n",
        "  print(model(I).shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScfzF-AyFFqN"
      },
      "source": [
        "# 15. 모델을 훈련하기 위한 train loop 를 작성하여 모델을 훈련시킵니다. loop 가 한 번만 성공적으로 동작하도록 해 보세요.\n",
        "\n",
        "* Loss: nn.MSELoss\n",
        "* Optimizer: optim.Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "tWna3nxoExjq"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3523.2159801136363"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_loss = 0\n",
        "for X_batch, y_batch in loader_train:\n",
        "    # print(X.shape), print(y.shape)\n",
        "    # break\n",
        "    y_pred = model(X_batch)\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_fn(y_pred, y_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # print(loss.item())\n",
        "    total_loss += loss.item()    \n",
        "    # break\n",
        "total_loss/len(loader_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUJmZw0MHD6V"
      },
      "source": [
        "# 16. epochs=200 으로 모델을 훈련시킵니다. 한 번의 epoch 마다 loss 값을 출력합니다.\n",
        "\n",
        "* GPU 를 사용할 수 있으면 적극 활용하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "911aXU5AGw8h",
        "outputId": "28ac2899-7964-4471-95de-63a3d65480f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0, loss: 3236.905013760653\n",
            "epoch: 1, loss: 3063.2038796164775\n",
            "epoch: 2, loss: 2939.834831099077\n",
            "epoch: 3, loss: 2815.18633034446\n",
            "epoch: 4, loss: 2739.712839577415\n",
            "epoch: 5, loss: 2617.8219360351563\n",
            "epoch: 6, loss: 2512.6943437056107\n",
            "epoch: 7, loss: 2421.0909831653944\n",
            "epoch: 8, loss: 2343.715830300071\n",
            "epoch: 9, loss: 2268.8934059836647\n",
            "epoch: 10, loss: 2181.315106756037\n",
            "epoch: 11, loss: 2111.2052301580256\n",
            "epoch: 12, loss: 2039.9076538085938\n",
            "epoch: 13, loss: 1953.853456809304\n",
            "epoch: 14, loss: 1879.926063676314\n",
            "epoch: 15, loss: 1812.7660361550072\n",
            "epoch: 16, loss: 1775.7331787109374\n",
            "epoch: 17, loss: 1701.757217129794\n",
            "epoch: 18, loss: 1639.781355979226\n",
            "epoch: 19, loss: 1565.3500516024503\n",
            "epoch: 20, loss: 1515.217129794034\n",
            "epoch: 21, loss: 1451.7767373518511\n",
            "epoch: 22, loss: 1423.5840504039418\n",
            "epoch: 23, loss: 1358.9342029918323\n",
            "epoch: 24, loss: 1319.7394503506748\n",
            "epoch: 25, loss: 1253.882821932706\n",
            "epoch: 26, loss: 1204.2210369852455\n",
            "epoch: 27, loss: 1161.4688322587447\n",
            "epoch: 28, loss: 1128.6215823086825\n",
            "epoch: 29, loss: 1101.6061332009056\n",
            "epoch: 30, loss: 1046.5838611949573\n",
            "epoch: 31, loss: 1017.07133733576\n",
            "epoch: 32, loss: 969.7769975142046\n",
            "epoch: 33, loss: 929.0559387207031\n",
            "epoch: 34, loss: 891.7227844238281\n",
            "epoch: 35, loss: 865.1089324951172\n",
            "epoch: 36, loss: 823.3359644109553\n",
            "epoch: 37, loss: 796.0847084738991\n",
            "epoch: 38, loss: 756.5642350630327\n",
            "epoch: 39, loss: 739.3844759854403\n",
            "epoch: 40, loss: 705.4463217995384\n",
            "epoch: 41, loss: 676.9606266368519\n",
            "epoch: 42, loss: 647.5530202692205\n",
            "epoch: 43, loss: 625.0426567771218\n",
            "epoch: 44, loss: 602.2444163929332\n",
            "epoch: 45, loss: 577.2558280251243\n",
            "epoch: 46, loss: 550.2569469082762\n",
            "epoch: 47, loss: 542.7383475563743\n",
            "epoch: 48, loss: 506.75855922698975\n",
            "epoch: 49, loss: 491.6208461414684\n",
            "epoch: 50, loss: 475.5178781682795\n",
            "epoch: 51, loss: 447.18955632990054\n",
            "epoch: 52, loss: 426.4253754233772\n",
            "epoch: 53, loss: 411.14447812167083\n",
            "epoch: 54, loss: 392.59464152943\n",
            "epoch: 55, loss: 372.1305903434753\n",
            "epoch: 56, loss: 363.58262953324754\n",
            "epoch: 57, loss: 345.536733453924\n",
            "epoch: 58, loss: 328.5560939095237\n",
            "epoch: 59, loss: 314.82873112071644\n",
            "epoch: 60, loss: 295.80755286650225\n",
            "epoch: 61, loss: 280.19901899857956\n",
            "epoch: 62, loss: 269.1857717340643\n",
            "epoch: 63, loss: 255.87569822831588\n",
            "epoch: 64, loss: 237.58303781409154\n",
            "epoch: 65, loss: 227.44730855768378\n",
            "epoch: 66, loss: 214.54201452081853\n",
            "epoch: 67, loss: 202.53495257618752\n",
            "epoch: 68, loss: 193.76322947415437\n",
            "epoch: 69, loss: 184.72969245910645\n",
            "epoch: 70, loss: 171.84841579090465\n",
            "epoch: 71, loss: 162.49110830480402\n",
            "epoch: 72, loss: 153.48856389847668\n",
            "epoch: 73, loss: 144.828709367324\n",
            "epoch: 74, loss: 136.7247530094602\n",
            "epoch: 75, loss: 128.92885615175422\n",
            "epoch: 76, loss: 123.8807805408131\n",
            "epoch: 77, loss: 115.54535792957653\n",
            "epoch: 78, loss: 108.47446564761076\n",
            "epoch: 79, loss: 103.91197204589844\n",
            "epoch: 80, loss: 96.3252517006614\n",
            "epoch: 81, loss: 88.9776903772896\n",
            "epoch: 82, loss: 84.50638686960394\n",
            "epoch: 83, loss: 79.12418236298994\n",
            "epoch: 84, loss: 73.3094854965806\n",
            "epoch: 85, loss: 69.05924228321422\n",
            "epoch: 86, loss: 65.55693144364791\n",
            "epoch: 87, loss: 60.45789794921875\n",
            "epoch: 88, loss: 58.24133946028623\n",
            "epoch: 89, loss: 52.19048813716932\n",
            "epoch: 90, loss: 49.61544473604722\n",
            "epoch: 91, loss: 45.943017181483185\n",
            "epoch: 92, loss: 41.9757237279957\n",
            "epoch: 93, loss: 38.97626813948155\n",
            "epoch: 94, loss: 36.03420277264985\n",
            "epoch: 95, loss: 33.65884302312678\n",
            "epoch: 96, loss: 31.05341607548974\n",
            "epoch: 97, loss: 28.931419357386503\n",
            "epoch: 98, loss: 26.413918831131674\n",
            "epoch: 99, loss: 24.146398771892894\n",
            "epoch: 100, loss: 22.185211674733594\n",
            "epoch: 101, loss: 20.698675232583827\n",
            "epoch: 102, loss: 18.872479678284037\n",
            "epoch: 103, loss: 17.411996572667903\n",
            "epoch: 104, loss: 15.922801157561215\n",
            "epoch: 105, loss: 14.650639992410486\n",
            "epoch: 106, loss: 13.466299659555608\n",
            "epoch: 107, loss: 12.979126093062488\n",
            "epoch: 108, loss: 11.567154119773345\n",
            "epoch: 109, loss: 10.680248898945072\n",
            "epoch: 110, loss: 9.752128881216048\n",
            "epoch: 111, loss: 9.113630836931142\n",
            "epoch: 112, loss: 8.310356221280315\n",
            "epoch: 113, loss: 7.756993835893544\n",
            "epoch: 114, loss: 7.152867074310779\n",
            "epoch: 115, loss: 6.659548289396546\n",
            "epoch: 116, loss: 6.221020160479979\n",
            "epoch: 117, loss: 5.643379778211767\n",
            "epoch: 118, loss: 5.249664880470796\n",
            "epoch: 119, loss: 4.867495787414637\n",
            "epoch: 120, loss: 4.484564598852938\n",
            "epoch: 121, loss: 4.446844018047506\n",
            "epoch: 122, loss: 4.258114360408349\n",
            "epoch: 123, loss: 3.6974768259308557\n",
            "epoch: 124, loss: 3.374868155880408\n",
            "epoch: 125, loss: 3.199513937668367\n",
            "epoch: 126, loss: 3.04728969335556\n",
            "epoch: 127, loss: 2.974869793924418\n",
            "epoch: 128, loss: 2.7702193493192846\n",
            "epoch: 129, loss: 2.6599355212666773\n",
            "epoch: 130, loss: 2.458927776596763\n",
            "epoch: 131, loss: 2.329874984107234\n",
            "epoch: 132, loss: 2.279056258093227\n",
            "epoch: 133, loss: 2.200318696553057\n",
            "epoch: 134, loss: 2.1216869878497993\n",
            "epoch: 135, loss: 2.0744625266302714\n",
            "epoch: 136, loss: 2.0404463134028696\n",
            "epoch: 137, loss: 2.2520336582579397\n",
            "epoch: 138, loss: 1.8692758775570175\n",
            "epoch: 139, loss: 1.8226359730417079\n",
            "epoch: 140, loss: 1.8199224350127308\n",
            "epoch: 141, loss: 1.9070391034538097\n",
            "epoch: 142, loss: 1.8510645445097575\n",
            "epoch: 143, loss: 1.6797747060656547\n",
            "epoch: 144, loss: 1.6218334157358516\n",
            "epoch: 145, loss: 1.5846757612445137\n",
            "epoch: 146, loss: 1.5543251496147026\n",
            "epoch: 147, loss: 1.505173631147905\n",
            "epoch: 148, loss: 1.60943394520066\n",
            "epoch: 149, loss: 1.4526253528131003\n",
            "epoch: 150, loss: 1.4733866194432432\n",
            "epoch: 151, loss: 1.501861271533099\n",
            "epoch: 152, loss: 1.452074930342761\n",
            "epoch: 153, loss: 1.569884181022644\n",
            "epoch: 154, loss: 1.3723551880229603\n",
            "epoch: 155, loss: 1.374686345745894\n",
            "epoch: 156, loss: 1.3774477644400163\n",
            "epoch: 157, loss: 1.3994157619097016\n",
            "epoch: 158, loss: 1.3011719608848744\n",
            "epoch: 159, loss: 1.3885868232358585\n",
            "epoch: 160, loss: 1.311035164242441\n",
            "epoch: 161, loss: 1.2762379608371042\n",
            "epoch: 162, loss: 1.3052880251949484\n",
            "epoch: 163, loss: 1.3154226350513372\n",
            "epoch: 164, loss: 1.3043318929997358\n",
            "epoch: 165, loss: 1.286388695781881\n",
            "epoch: 166, loss: 1.228504576059905\n",
            "epoch: 167, loss: 1.2416280023753643\n",
            "epoch: 168, loss: 1.2489210456609725\n",
            "epoch: 169, loss: 1.2110645834695208\n",
            "epoch: 170, loss: 1.3103976293043658\n",
            "epoch: 171, loss: 1.2387089669704436\n",
            "epoch: 172, loss: 1.2349229365587235\n",
            "epoch: 173, loss: 1.342226819287647\n",
            "epoch: 174, loss: 1.2528307286175815\n",
            "epoch: 175, loss: 1.2379234642467716\n",
            "epoch: 176, loss: 1.1750968318093906\n",
            "epoch: 177, loss: 1.2087135152383284\n",
            "epoch: 178, loss: 1.192676597901366\n",
            "epoch: 179, loss: 1.152874821153554\n",
            "epoch: 180, loss: 1.2161854031411083\n",
            "epoch: 181, loss: 1.194412176175551\n",
            "epoch: 182, loss: 1.1839376857334918\n",
            "epoch: 183, loss: 1.2029057827862826\n",
            "epoch: 184, loss: 1.199622498994524\n",
            "epoch: 185, loss: 1.2544709325514056\n",
            "epoch: 186, loss: 1.3015776520425624\n",
            "epoch: 187, loss: 1.203665647723458\n",
            "epoch: 188, loss: 1.2743139539252628\n",
            "epoch: 189, loss: 1.2540580278093165\n",
            "epoch: 190, loss: 1.1974700476635587\n",
            "epoch: 191, loss: 1.1818406180901961\n",
            "epoch: 192, loss: 1.2568694987080313\n",
            "epoch: 193, loss: 1.1388312250375747\n",
            "epoch: 194, loss: 1.1400896168567918\n",
            "epoch: 195, loss: 1.2294777272777124\n",
            "epoch: 196, loss: 1.2108906651085074\n",
            "epoch: 197, loss: 1.1820042756470768\n",
            "epoch: 198, loss: 1.1636806100606918\n",
            "epoch: 199, loss: 1.2599444778128104\n"
          ]
        }
      ],
      "source": [
        "epochs = 200\n",
        "for epoch in range(epochs):\n",
        "  total_loss = 0\n",
        "  for X_batch, y_batch in loader_train:\n",
        "      # print(X.shape), print(y.shape)\n",
        "      # break\n",
        "      y_pred = model(X_batch)\n",
        "      optimizer.zero_grad()\n",
        "      loss = loss_fn(y_pred, y_batch)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      # print(loss.item())\n",
        "      total_loss += loss.item()    \n",
        "      # break\n",
        "\n",
        "  epoch_loss = total_loss / len(loader_train)\n",
        "  print(f'epoch: {epoch}, loss: {epoch_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfHzl8EmO4fD"
      },
      "source": [
        "# 17. 실제 값과 훈련된 모델의 예측값을 plot 으로 그려봅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x20aed693f50>]"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval()\n",
        "\n",
        "X_tensor = torch.tensor(X).float()\n",
        "with torch.no_grad():\n",
        "  y_pred = model(X_tensor).numpy()\n",
        "  #y_pred = model(X_tensor).cpu().numpy()\n",
        "\n",
        "plt.plot(df.index[seq_len:], y)\n",
        "plt.plot(df.index[seq_len:], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcPfJKByPBxT"
      },
      "source": [
        "# 18. 모델의 성능을 MSE, RMSE, MAE, R2 메트릭을 이용하여 측정해 봅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF7EoWRXJjGQ",
        "outputId": "bc148b61-a823-44b8-b106-d2ff57c4ebf6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'MSE: 1.36859, RMSE: 1.16987, MAE: 0.78632, R2: 0.99844'"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y, y_pred)\n",
        "r2 = r2_score(y, y_pred)\n",
        "\n",
        "f\"MSE: {mse:.5f}, RMSE: {rmse:.5f}, MAE: {mae:.5f}, R2: {r2:.5f}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d41xsezKPMUm"
      },
      "source": [
        "# 19. 12번에서 생성한 데이터를 이용하여 다음을 수행하시오.\n",
        "\n",
        "* sklearn 의 MinMaxScaler 를 이용하여 데이터를 정규화 합니다. (fit_transform)\n",
        "* 9:2 의 비율로 train/test 로 데이터를 split 하여, X_trian, X_test, y_train, y_test 변수에 저장합니다.\n",
        "* train/test 용 DataLoader 객체를 생성하여 loader_train, loader_test 변수에 저장합니다.\n",
        "* 모델 훈련 시에는 loader_train 을 이용하고, 모델의 예측력을 확인할 때는 loader_test 를 이용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(3522,)"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "ticker_symbol = \"SBUX\"\n",
        "df = yf.download(ticker_symbol, start=\"2010-01-01\", end=\"2023-12-31\")\n",
        "\n",
        "df.columns = ['Close', 'High', 'Low', 'Open', 'Volume']\n",
        "data = df['Close'].values\n",
        "\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "seq_len = 30\n",
        "X, y = [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(data)-seq_len) :\n",
        "    X.append(data_np[i:i+seq_len])\n",
        "    y.append(data_np[i+seq_len])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((3492, 30), (3492,))"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "minMaxScaler = MinMaxScaler()\n",
        "mmData = minMaxScaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = mmData.reshape(-1, 30, 1)\n",
        "y = y.reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((3492, 30, 1), (3492, 1))"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((3142, 30, 1), (3142, 1), (350, 30, 1), (350, 1))"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "X_train_tensor, y_train_tensor = torch.tensor(X_train).float(), torch.tensor(y_train).float()\n",
        "X_test_tensor, y_test_tensor = torch.tensor(X_test).float(), torch.tensor(y_test).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader_train = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=32, shuffle=True)\n",
        "loader_test = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StockPrediction(nn.Module):\n",
        "  def __init__(self, input_size=1, hidden_size=50, output_size=1):\n",
        "    super(StockPrediction, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "    self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    out, (h, c) = self.lstm(x)\n",
        "    out = self.linear(out[:, -1, :])\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = StockPrediction()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0, loss: 3327.554712160669\n",
            "epoch: 1, loss: 2618.2158190794667\n",
            "epoch: 2, loss: 2214.690171983507\n",
            "epoch: 3, loss: 1903.341806122751\n",
            "epoch: 4, loss: 1639.2933115333017\n",
            "epoch: 5, loss: 1450.2887067698468\n",
            "epoch: 6, loss: 1296.7813899492976\n",
            "epoch: 7, loss: 1178.7705663815893\n",
            "epoch: 8, loss: 1090.6683198562775\n",
            "epoch: 9, loss: 1022.2503686770044\n",
            "epoch: 10, loss: 975.0589981849747\n",
            "epoch: 11, loss: 937.0205226089015\n",
            "epoch: 12, loss: 919.5053427339805\n",
            "epoch: 13, loss: 902.8348216047191\n",
            "epoch: 14, loss: 895.9637062766335\n",
            "epoch: 15, loss: 894.1597530480587\n",
            "epoch: 16, loss: 887.612574105311\n",
            "epoch: 17, loss: 874.6437254626342\n",
            "epoch: 18, loss: 811.0086663756707\n",
            "epoch: 19, loss: 464.80543918802283\n",
            "epoch: 20, loss: 374.0980466495861\n",
            "epoch: 21, loss: 315.20065754591815\n",
            "epoch: 22, loss: 259.3022400827119\n",
            "epoch: 23, loss: 214.7005658390546\n",
            "epoch: 24, loss: 180.71853460446752\n",
            "epoch: 25, loss: 151.45257934416182\n",
            "epoch: 26, loss: 129.04572134306937\n",
            "epoch: 27, loss: 110.0558836580527\n",
            "epoch: 28, loss: 94.2074941384672\n",
            "epoch: 29, loss: 82.67974250485199\n",
            "epoch: 30, loss: 70.65317451110994\n",
            "epoch: 31, loss: 61.36083039852104\n",
            "epoch: 32, loss: 55.95419843269117\n",
            "epoch: 33, loss: 48.03790601094564\n",
            "epoch: 34, loss: 41.611936049027875\n",
            "epoch: 35, loss: 38.26087427620936\n",
            "epoch: 36, loss: 31.84110204137937\n",
            "epoch: 37, loss: 29.323870576993382\n",
            "epoch: 38, loss: 26.825798689716994\n",
            "epoch: 39, loss: 23.96641843487518\n",
            "epoch: 40, loss: 21.267593961773496\n",
            "epoch: 41, loss: 19.518672266391793\n",
            "epoch: 42, loss: 17.646790930719085\n",
            "epoch: 43, loss: 17.649831136067707\n",
            "epoch: 44, loss: 15.145026616375855\n",
            "epoch: 45, loss: 15.13175504135363\n",
            "epoch: 46, loss: 13.147413061122702\n",
            "epoch: 47, loss: 12.600732745546283\n",
            "epoch: 48, loss: 12.62075392646019\n",
            "epoch: 49, loss: 10.636176407939256\n",
            "epoch: 50, loss: 11.803631893312089\n",
            "epoch: 51, loss: 10.476926465227146\n",
            "epoch: 52, loss: 9.997991964070485\n",
            "epoch: 53, loss: 9.774191015898579\n",
            "epoch: 54, loss: 8.787315847897771\n",
            "epoch: 55, loss: 9.265134081696019\n",
            "epoch: 56, loss: 8.657307604346613\n",
            "epoch: 57, loss: 9.276356160038649\n",
            "epoch: 58, loss: 8.007197984541305\n",
            "epoch: 59, loss: 7.38659186074228\n",
            "epoch: 60, loss: 8.581015006460325\n",
            "epoch: 61, loss: 7.565680573685\n",
            "epoch: 62, loss: 8.254711300435693\n",
            "epoch: 63, loss: 8.593065391887318\n",
            "epoch: 64, loss: 6.819921291235722\n",
            "epoch: 65, loss: 6.382516747773296\n",
            "epoch: 66, loss: 6.246639620776128\n",
            "epoch: 67, loss: 6.253673708800114\n",
            "epoch: 68, loss: 6.308456379957874\n",
            "epoch: 69, loss: 6.095873579834446\n",
            "epoch: 70, loss: 6.004665932571045\n",
            "epoch: 71, loss: 5.328965419470662\n",
            "epoch: 72, loss: 6.490623240519052\n",
            "epoch: 73, loss: 5.765014159559\n",
            "epoch: 74, loss: 5.331299528931126\n",
            "epoch: 75, loss: 5.997550699445936\n",
            "epoch: 76, loss: 5.240856542129709\n",
            "epoch: 77, loss: 4.872280891495522\n",
            "epoch: 78, loss: 5.131977088523634\n",
            "epoch: 79, loss: 4.540704827115993\n",
            "epoch: 80, loss: 4.548151945826983\n",
            "epoch: 81, loss: 4.595305365745467\n",
            "epoch: 82, loss: 4.257968383606034\n",
            "epoch: 83, loss: 4.094301662059745\n",
            "epoch: 84, loss: 4.51393236174728\n",
            "epoch: 85, loss: 4.214605950345897\n",
            "epoch: 86, loss: 3.8466780312133557\n",
            "epoch: 87, loss: 3.97797346596766\n",
            "epoch: 88, loss: 3.878336226097261\n",
            "epoch: 89, loss: 4.196742557516002\n",
            "epoch: 90, loss: 4.062365317585493\n",
            "epoch: 91, loss: 3.535566307077504\n",
            "epoch: 92, loss: 3.4201370453593705\n",
            "epoch: 93, loss: 3.802768335197911\n",
            "epoch: 94, loss: 3.5543999876638854\n",
            "epoch: 95, loss: 3.514229345803309\n",
            "epoch: 96, loss: 3.417183190885216\n",
            "epoch: 97, loss: 3.3039163019922047\n",
            "epoch: 98, loss: 3.112648479866259\n",
            "epoch: 99, loss: 3.1921151069679645\n",
            "epoch: 100, loss: 3.2252444781438268\n",
            "epoch: 101, loss: 3.0859320187809494\n",
            "epoch: 102, loss: 3.313490745997188\n",
            "epoch: 103, loss: 3.0351886285675898\n",
            "epoch: 104, loss: 2.895370542702049\n",
            "epoch: 105, loss: 2.981711417737633\n",
            "epoch: 106, loss: 2.8595849552539865\n",
            "epoch: 107, loss: 2.7305336525343886\n",
            "epoch: 108, loss: 3.1895694317239704\n",
            "epoch: 109, loss: 3.238645404276222\n",
            "epoch: 110, loss: 2.631245450238989\n",
            "epoch: 111, loss: 2.507259808405481\n",
            "epoch: 112, loss: 2.453023387025101\n",
            "epoch: 113, loss: 2.49364079790886\n",
            "epoch: 114, loss: 2.554275309196626\n",
            "epoch: 115, loss: 2.5572241376144715\n",
            "epoch: 116, loss: 2.6727775317249876\n",
            "epoch: 117, loss: 2.280532302278461\n",
            "epoch: 118, loss: 2.553782134947151\n",
            "epoch: 119, loss: 2.592279984493448\n",
            "epoch: 120, loss: 2.9327191283004455\n",
            "epoch: 121, loss: 2.296337547326329\n",
            "epoch: 122, loss: 2.3332195504747255\n",
            "epoch: 123, loss: 2.198644783159699\n",
            "epoch: 124, loss: 2.148031509584851\n",
            "epoch: 125, loss: 2.6002828584174917\n",
            "epoch: 126, loss: 2.3003441769667345\n",
            "epoch: 127, loss: 2.4907651182376975\n",
            "epoch: 128, loss: 2.2737268131188673\n",
            "epoch: 129, loss: 2.2771114050739945\n",
            "epoch: 130, loss: 2.278052638275455\n",
            "epoch: 131, loss: 2.388905421651975\n",
            "epoch: 132, loss: 2.141677858251514\n",
            "epoch: 133, loss: 2.029356611196441\n",
            "epoch: 134, loss: 2.4300302706583583\n",
            "epoch: 135, loss: 2.0959553438605685\n",
            "epoch: 136, loss: 2.2306391906858694\n",
            "epoch: 137, loss: 2.4792488247457176\n",
            "epoch: 138, loss: 1.9929395847850375\n",
            "epoch: 139, loss: 2.1899516636675056\n",
            "epoch: 140, loss: 2.1602395706706576\n",
            "epoch: 141, loss: 1.9710546536277038\n",
            "epoch: 142, loss: 1.8628970330411738\n",
            "epoch: 143, loss: 1.8163665981605799\n",
            "epoch: 144, loss: 2.0480219336471173\n",
            "epoch: 145, loss: 1.974029429633208\n",
            "epoch: 146, loss: 1.971820994760051\n",
            "epoch: 147, loss: 2.0305929521117547\n",
            "epoch: 148, loss: 2.0321929159790577\n",
            "epoch: 149, loss: 1.8195050066769725\n",
            "epoch: 150, loss: 1.9343951242138642\n",
            "epoch: 151, loss: 1.8655721159896466\n",
            "epoch: 152, loss: 1.7798903742522905\n",
            "epoch: 153, loss: 2.193456956834504\n",
            "epoch: 154, loss: 1.6898093548688022\n",
            "epoch: 155, loss: 1.728359355167909\n",
            "epoch: 156, loss: 1.928830082368369\n",
            "epoch: 157, loss: 2.062119881613086\n",
            "epoch: 158, loss: 1.73305878825862\n",
            "epoch: 159, loss: 1.95962878730562\n",
            "epoch: 160, loss: 1.8100791040695074\n",
            "epoch: 161, loss: 1.7787722728469155\n",
            "epoch: 162, loss: 1.887617385748661\n",
            "epoch: 163, loss: 2.0782971857774135\n",
            "epoch: 164, loss: 1.7065713694601348\n",
            "epoch: 165, loss: 1.733346559182562\n",
            "epoch: 166, loss: 1.778037601649159\n",
            "epoch: 167, loss: 1.9466236360145337\n",
            "epoch: 168, loss: 1.6464413049245121\n",
            "epoch: 169, loss: 1.7130982665100483\n",
            "epoch: 170, loss: 1.7013568471778522\n",
            "epoch: 171, loss: 1.9105835556983948\n",
            "epoch: 172, loss: 1.6701727672056719\n",
            "epoch: 173, loss: 1.7520100746040392\n",
            "epoch: 174, loss: 1.752144772898067\n",
            "epoch: 175, loss: 1.653079945932735\n",
            "epoch: 176, loss: 1.8312741939467614\n",
            "epoch: 177, loss: 1.7900297714002205\n",
            "epoch: 178, loss: 1.7317431858091643\n",
            "epoch: 179, loss: 1.6914775853205208\n",
            "epoch: 180, loss: 1.6102079816087327\n",
            "epoch: 181, loss: 1.540166222988957\n",
            "epoch: 182, loss: 1.6071451426756502\n",
            "epoch: 183, loss: 1.663895602178092\n",
            "epoch: 184, loss: 1.5263407955868076\n",
            "epoch: 185, loss: 1.7791143887572818\n",
            "epoch: 186, loss: 1.5950390913269736\n",
            "epoch: 187, loss: 1.4883065238745525\n",
            "epoch: 188, loss: 1.643680612547229\n",
            "epoch: 189, loss: 1.8180848345009968\n",
            "epoch: 190, loss: 1.6654818757164358\n",
            "epoch: 191, loss: 1.5484355886777241\n",
            "epoch: 192, loss: 1.6683560445754215\n",
            "epoch: 193, loss: 1.522179988297549\n",
            "epoch: 194, loss: 1.670256870563584\n",
            "epoch: 195, loss: 1.4100783425148087\n",
            "epoch: 196, loss: 1.6538936047240942\n",
            "epoch: 197, loss: 1.4954557197563576\n",
            "epoch: 198, loss: 1.4380167933425518\n",
            "epoch: 199, loss: 1.5514679120646582\n"
          ]
        }
      ],
      "source": [
        "epochs = 200\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "  total_loss = 0\n",
        "  for X_batch, y_batch in loader_train:\n",
        "      # print(X.shape), print(y.shape)\n",
        "      # break\n",
        "      out = model(X_batch)\n",
        "      loss = loss_fn(out, y_batch)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      # print(loss.item())\n",
        "      total_loss += loss.item()    \n",
        "      # break\n",
        "\n",
        "  epoch_loss = total_loss / len(loader_train)\n",
        "  print(f'epoch: {epoch}, loss: {epoch_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "imOSrKADRhOr"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "y_pred_list = []\n",
        "with torch.no_grad():\n",
        "  for X_batch, _ in loader_test:\n",
        "    y_pred = model(X_batch).numpy()\n",
        "    #y_pred = model(X_batch).cpu().numpy()\n",
        "    y_pred_list.append(y_pred)\n",
        "\n",
        "y_pred = np.concatenate(y_pred_list, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "04vRjZSyQ28r",
        "outputId": "03f5ec01-9fd1-4d59-cce9-1f3dd2ad3b78"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "non-broadcastable output operand with shape (350,1) doesn't match the broadcast shape (350,30)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[143], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m\n\u001b[1;32m----> 3\u001b[0m y_test_ori \u001b[38;5;241m=\u001b[39m \u001b[43mminMaxScaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m y_pred_ori \u001b[38;5;241m=\u001b[39m minMaxScaler\u001b[38;5;241m.\u001b[39minverse_transform(y_pred)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mxticks(rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m45\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch_book\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:581\u001b[0m, in \u001b[0;36mMinMaxScaler.inverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    571\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m    573\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    574\u001b[0m     X,\n\u001b[0;32m    575\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    578\u001b[0m     ensure_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    579\u001b[0m )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_\u001b[49m\n\u001b[0;32m    582\u001b[0m X \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
            "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (350,1) doesn't match the broadcast shape (350,30)"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "train_size = 0.9\n",
        "y_test_ori = minMaxScaler.inverse_transform(y_test)\n",
        "y_pred_ori = minMaxScaler.inverse_transform(y_pred)\n",
        "\n",
        "plt.xticks(rotation=45)\n",
        "plt.plot(df.index[train_size + seq_len:], y_test_ori)\n",
        "plt.plot(df.index[train_size + seq_len:], y_pred_ori)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zETuhTcKQ4Wz",
        "outputId": "2aa0eee9-9cd4-4d17-d53f-956bfee43851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE: 2.65828, RMSE: 1.63042, MAE: 1.14909, R2: 0.94367\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "mse = mean_squared_error(y_test_ori, y_pred_ori)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test_ori, y_pred_ori)\n",
        "r2 = r2_score(y_test_ori, y_pred_ori)\n",
        "\n",
        "print(f\"MSE: {mse:.5f}, RMSE: {rmse:.5f}, MAE: {mae:.5f}, R2: {r2:.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZII2s9VgSmkl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch_book",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
